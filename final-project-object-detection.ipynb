{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download Libraries","metadata":{}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport shutil\nimport yaml\nfrom ultralytics import YOLO\nimport pandas as pd\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Changing Data to YOLO Format & Data Augmentation\nMy method is duplicating to x2 the number of labels in Banana, Pineapple, Watermelon Object. And about Orange Object i just take a hafl of it's original data. So you can see at Below Section: Count labels per... I have shown all the number of labels per Object after processing data.","metadata":{}},{"cell_type":"markdown","source":"Processing file can find at :  https://www.kaggle.com/datasets/bithlong/processing-data  \nBest.pt can find at         :  https://www.kaggle.com/datasets/bithlong/trained-yolo-model  \nFinal Dataset can be find at:  https://www.kaggle.com/datasets/bithlong/final-pro-dataset  \n","metadata":{}},{"cell_type":"markdown","source":"# Split Train and Val Dataset","metadata":{}},{"cell_type":"code","source":"src_images_dir = '/kaggle/input/final-pro-dataset/yolo_dataset/new_train/images'\nsrc_labels_dir = '/kaggle/input/final-pro-dataset/yolo_dataset/new_train/labels'\n\nimages = [f for f in os.listdir(src_images_dir) if f.endswith('.jpg')]\ntrain_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n\n# Create new train/val directories\nos.makedirs('/kaggle/working/yolo_dataset/train/images', exist_ok=True)\nos.makedirs('/kaggle/working/yolo_dataset/train/labels', exist_ok=True)\nos.makedirs('/kaggle/working/yolo_dataset/val/images', exist_ok=True)\nos.makedirs('/kaggle/working/yolo_dataset/val/labels', exist_ok=True)\n\n# Copy files\nfor img in images:\n    shutil.copy(f'/kaggle/input/final-pro-dataset/yolo_dataset/new_train/images/{img}', f'/kaggle/working/yolo_dataset/train/images/{img}')\n    shutil.copy(f'/kaggle/input/final-pro-dataset/yolo_dataset/new_train/labels/{img.replace(\".jpg\", \".txt\")}', f'/kaggle/working/yolo_dataset/train/labels/{img.replace(\".jpg\", \".txt\")}')\nfor img in val_imgs:\n    shutil.copy(f'/kaggle/input/final-pro-dataset/yolo_dataset/new_train/images/{img}', f'/kaggle/working/yolo_dataset/val/images/{img}')\n    shutil.copy(f'/kaggle/input/final-pro-dataset/yolo_dataset/new_train/labels/{img.replace(\".jpg\", \".txt\")}', f'/kaggle/working/yolo_dataset/val/labels/{img.replace(\".jpg\", \".txt\")}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create .ymal Configuration File","metadata":{}},{"cell_type":"code","source":"data_yaml = {\n    'train': '/kaggle/working/yolo_dataset/train/images',\n    'val' : '/kaggle/working/yolo_dataset/val/images',\n    'test': '/kaggle/input/final-pro-dataset/yolo_dataset/test/images',\n    'nc': 6,\n    'names': {\n        0: \"Apple\",\n        1: \"Banana\",\n        2: \"Grapes\",\n        3: \"Orange\",\n        4: \"Pineapple\",\n        5: \"Watermelon\"\n    }\n}\n\nwith open('/kaggle/working/data.yaml', 'w') as f:\n    yaml.dump(data_yaml, f)\n\n# Verify\n!cat /kaggle/working/data.yaml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:10:20.554057Z","iopub.execute_input":"2025-06-11T05:10:20.554787Z","iopub.status.idle":"2025-06-11T05:10:20.692707Z","shell.execute_reply.started":"2025-06-11T05:10:20.554755Z","shell.execute_reply":"2025-06-11T05:10:20.692006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Count labels per class to check for imbalance:","metadata":{}},{"cell_type":"markdown","source":"Training Dataset","metadata":{}},{"cell_type":"code","source":"# import os\n# from collections import Counter\n\n# train_class_counts = Counter()\n# for label_file in os.listdir('/kaggle/input/final-pro-dataset/yolo_dataset/new_train/labels'):\n#     with open(f'/kaggle/input/final-pro-dataset/yolo_dataset/new_train/labels/{label_file}') as f:\n#         for line in f:\n#             class_id = line.split()[0]\n#             train_class_counts[class_id] += 1\n# print(train_class_counts)\n# # Counter({'3': 7146, '0': 6771, '2': 6605, '1': 6332, '5': 3440, '4': 2838})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T04:44:09.043627Z","iopub.execute_input":"2025-06-11T04:44:09.043865Z","iopub.status.idle":"2025-06-11T04:44:49.849326Z","shell.execute_reply.started":"2025-06-11T04:44:09.043840Z","shell.execute_reply":"2025-06-11T04:44:49.848561Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training Model\nThe model that i used is YOLO V11 Medium","metadata":{}},{"cell_type":"code","source":"# Load pre-trained model\nmodel = YOLO('yolo11m.pt') # \n\n# Train\nmodel.train(\n    # === BASIC SETTINGS ===\n    data='/kaggle/working/data.yaml',\n    epochs=100,              # Increased for larger dataset\n    imgsz=640,\n    batch=16,                # Increased batch size for better gradient estimates\n    device=0,\n    \n    # === LEARNING RATE & OPTIMIZATION ===\n    lr0=0.0005,              # Lower initial learning rate for stability\n    lrf=0.001,               # Final learning rate\n    momentum=0.937,         # Standard momentum\n    weight_decay=0.0005,    # L2 regularization\n    warmup_epochs=5,        # More warmup for larger dataset\n    warmup_momentum=0.8,\n    warmup_bias_lr=0.1,\n    optimizer='AdamW',\n    \n    # === LOSS WEIGHTS (adjusted for class imbalance) ===\n    box=0.1,                # Bounding box loss weight\n    cls=0.7,                # REDUCED classification loss (you have good balance now)\n    dfl=1.5,                # Distribution focal loss\n    \n    # # === DATA AUGMENTATION (optimized for 19k images) ===\n    # # Geometric augmentations\n    # degrees=15,             # Reduced rotation (fruits have natural orientation)\n    # translate=0.1,          # Small translation\n    # scale=0.8,              # More aggressive scaling\n    # shear=3.0,              # Shear transformation\n    # perspective=0.0001,     # Very small perspective change\n    \n    # Flip augmentations\n    flipud=0.0,             # NO vertical flip (fruits orientation matters)\n    fliplr=0.0,             # Keep horizontal flip\n    \n    # Advanced augmentations\n    # mosaic=1.0,             # Full mosaic (great for detection)\n    mixup=0.15,              # Light mixup for robustness\n    copy_paste=0.2,         # Copy-paste augmentation\n    \n    # # Color augmentations (important for fruits!)\n    # hsv_h=0.008,            # Subtle hue changes\n    # hsv_s=0.6,              # Moderate saturation changes\n    # hsv_v=0.3,              # Brightness variations\n    \n    # === TRAINING STRATEGY ===\n    patience=25,            # Early stopping patience\n    save_period=10,         # Save every 10 epochs\n    val=True,               # Enable validation\n    plots=True,             # Generate training plots\n    \n    # === ADVANCED SETTINGS ===\n    amp=True,               # Automatic Mixed Precision (faster training)\n    fraction=1.0,           # Use full dataset\n    profile=False,          # Set True for performance profiling\n    \n    # === OUTPUT ===\n    name='fruit_detection_balanced_v2',\n    exist_ok=True,\n    \n    # === ADDITIONAL OPTIMIZATIONS ===\n    close_mosaic=10,        # Stop mosaic in last 10 epochs for better final training\n    resume=False,           # Set to checkpoint path if resuming\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T05:57:36.120134Z","iopub.execute_input":"2025-06-11T05:57:36.120889Z","iopub.status.idle":"2025-06-11T14:06:29.626001Z","shell.execute_reply.started":"2025-06-11T05:57:36.120862Z","shell.execute_reply":"2025-06-11T14:06:29.624596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test Model on Validation Dataset","metadata":{}},{"cell_type":"code","source":"results = model.val()\nprint(f\"mAP@0.5: {results.box.map}\")\nfor i, ap in enumerate(results.box.ap):\n    print(f\"AP@0.5 for {results.names[i]}: {ap}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generate Prediction on Test Dataset & Save to Submission.csv\n","metadata":{}},{"cell_type":"markdown","source":"The result on 30% is 0.76 which is pretty low comparing to mAP the model have shown in training process. Which mean the model is so-called overfitted","metadata":{}},{"cell_type":"code","source":"def generate_submission_csv(model_path, test_images_dir, submission_path=\"submission.csv\"):\n    \"\"\"\n    Tạo file submission \n    \n    Args:\n        model_path (str): Đường dẫn đến file weight của mô hình YOLO.\n        test_images_dir (str): Thư mục chứa ảnh test.\n        submission_path (str): Đường dẫn lưu file CSV output.\n    \"\"\"\n    model = YOLO(model_path)\n    image_files = [f for f in os.listdir(test_images_dir) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n\n    submission_data = []\n\n    for image_file in image_files:\n        img_path = os.path.join(test_images_dir, image_file)\n\n        result = model(img_path, conf=0.1, iou=0.5, verbose=False)[0]\n\n        predictions = []\n        for box in result.boxes:\n            xyxy = box.xyxy[0].tolist()\n            cls_id = int(box.cls[0])\n            conf = float(box.conf[0])\n            w = xyxy[2] - xyxy[0]\n            h = xyxy[3] - xyxy[1]\n\n            if conf >= 0.01 and w * h > 50:  \n                predictions.append({\n                    \"x_min\": round(xyxy[0], 2),\n                    \"y_min\": round(xyxy[1], 2),\n                    \"x_max\": round(xyxy[2], 2),\n                    \"y_max\": round(xyxy[3], 2),\n                    \"class\": cls_id,\n                    \"confidence\": round(conf, 3)\n                })\n\n        image_id = os.path.splitext(image_file)[0]\n        formatted_id = f\"img{int(image_id)}\" if image_id.isdigit() else image_id\n\n        submission_data.append({\n            \"ID\": formatted_id,\n            \"bounding_boxes\": json.dumps(predictions)\n        })\n\n    df = pd.DataFrame(submission_data)\n    df.to_csv(submission_path, index=False)\n    print(f\"Submission file saved to {submission_path}\")\n\n\ngenerate_submission_csv(\n    model_path=\"/kaggle/input/trained-yolo-model/best.pt\",\n    test_images_dir=\"/kaggle/input/final-pro-dataset/yolo_dataset/test/images\",\n    submission_path=\"/kaggle/working/submission.csv\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T15:32:39.189439Z","iopub.execute_input":"2025-06-11T15:32:39.189662Z","iopub.status.idle":"2025-06-11T15:33:04.095315Z","shell.execute_reply.started":"2025-06-11T15:32:39.189644Z","shell.execute_reply":"2025-06-11T15:33:04.094478Z"}},"outputs":[],"execution_count":null}]}